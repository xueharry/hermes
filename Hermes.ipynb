{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hermes\n",
    "Sequence to sequence chatbot trained on Facebook messenger data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Activation, TimeDistributed, Dense, RepeatVector, Dropout, recurrent\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "\n",
    "from random import sample\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Configuration(object):\n",
    "    \"\"\"Class for holding configuration settings\"\"\"\n",
    "    \n",
    "CONFIG = Configuration()\n",
    "CONFIG.max_input_len = 10 # Maximum number of timesteps in an input sequence\n",
    "CONFIG.max_output_len = 10 # Maximum number of timesteps in an output sequence (9 for words, 1 for <EOS>)\n",
    "UNKNOWN_TOKEN = \"<UNKNOWN>\"\n",
    "EOS = \"<EOS>\"\n",
    "\n",
    "# Training parameters\n",
    "CONFIG.batch_size = 100\n",
    "CONFIG.epochs = 500\n",
    "\n",
    "CONFIG.amount_of_dropout = 0.1\n",
    "CONFIG.hidden_size = 50 # 500\n",
    "CONFIG.initialization = \"he_normal\" # Gaussian initialization scaled by fan-in (He et al., 2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Facebook Messenger messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read html\n",
    "page = urllib.urlopen('data/messages.htm').read()\n",
    "soup = BeautifulSoup(page, \"html5lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_div = soup.body.div.next_sibling\n",
    "# Get div of room groupchat\n",
    "groupchat_div = contents_div.div.div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of all messages in this groupchat\n",
    "groupchat_messages = []\n",
    "\n",
    "groupchat_p = groupchat_div.find_all(\"p\")\n",
    "for p in groupchat_p:\n",
    "    groupchat_messages.append(p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of messages: 9257\n",
      "[u'leggo', u'hebrew bible leffo', u'looks like culture and belief is all that is left', u'Ez', u'workload is only ~9.4 hours per week we can do this guys', u'graduate level probability', u\"stat 210 let's go\", u'yah', u'different instructor though', u'according to my.harvard']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Clean\n",
    "print \"Number of messages:\", len(groupchat_messages)\n",
    "print groupchat_messages[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8255\n"
     ]
    }
   ],
   "source": [
    "# Filter for messages < maximum input length\n",
    "filtered = []\n",
    "for message in groupchat_messages:\n",
    "    if len(message.split(\" \")) < CONFIG.max_output_len:\n",
    "        filtered.append(message)\n",
    "\n",
    "groupchat_messages = filtered\n",
    "print(len(groupchat_messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create pickle dump of all messages\n",
    "pickle.dump(groupchat_messages, open('data/groupchat.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list of unique tokens used\n",
    "tokens = []\n",
    "for message in groupchat_messages:\n",
    "    tokens += message.split(' ')\n",
    "tokens = list(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create question, answer pairs using alternating pairs of messages\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for i in range(len(groupchat_messages) - 1):\n",
    "    questions.append(groupchat_messages[i].split(\" \"))\n",
    "    answers.append(groupchat_messages[i + 1].split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Language(object):\n",
    "    def __init__(self, tokens, maxlen):\n",
    "        # Add STOP and UNKNOWN_TOKEN to vocabulary\n",
    "        tokens.append(UNKNOWN_TOKEN)\n",
    "        self.tokens = sorted(set(tokens))\n",
    "        # Reserve index 0 for the EOS token\n",
    "        self.tokens_indices = dict((t, i + 1) for i, t in enumerate(self.tokens))\n",
    "        self.indices_tokens = dict((i + 1, t) for i, t in enumerate(self.tokens))\n",
    "        self.tokens.insert(0, EOS)\n",
    "        self.tokens_indices[EOS] = 0\n",
    "        self.indices_tokens[0] = EOS\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        \"\"\"The number of unique tokens\"\"\"\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def encode(self, l):\n",
    "        \"\"\"Encode a list of tokens as one-hot\"\"\"\n",
    "        X = np.zeros((self.maxlen, self.size), dtype=np.bool)\n",
    "        for i, item in enumerate(l):\n",
    "            try:\n",
    "                X[i, self.tokens_indices[item]] = 1\n",
    "            except KeyError:\n",
    "                X[i, self.tokens_indices[UNKNOWN_TOKEN]] = 1\n",
    "        # Insert EOS token\n",
    "        X[i + 1, self.tokens_indices[EOS]] = 1\n",
    "        return X\n",
    "\n",
    "    def decode(self, X):\n",
    "        \"\"\"Decode array of predicted token indices into a array of predicted tokens\"\"\"\n",
    "        result = []\n",
    "        for x in X:\n",
    "            if self.indices_tokens[x] == EOS:\n",
    "                return result\n",
    "            result.append(self.indices_tokens[x])\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def most_likely_seq_probs(self, X):\n",
    "        \"\"\"Return vector of probabilities for the most likely sequence\"\"\"\n",
    "        probs = np.array([])\n",
    "\n",
    "        for x in X:\n",
    "            most_likely_ind = x.argmax()\n",
    "            most_likely_prob = x.max()\n",
    "\n",
    "            # Check for end-of-line token and return if found\n",
    "            if most_likely_ind == self.tokens_indices[EOS]:\n",
    "                return probs\n",
    "            else:\n",
    "                probs = np.append(probs, most_likely_prob)\n",
    "\n",
    "        return probs\n",
    "\n",
    "def generate_sample_weights(Y):\n",
    "    sample_weights = np.zeros((len(Y), CONFIG.max_output_len))\n",
    "\n",
    "    for i in range(len(Y)):\n",
    "        for j in range(CONFIG.max_output_len):\n",
    "            if np.any(Y[i, j]):\n",
    "                sample_weights[i,j] = 1\n",
    "\n",
    "    return sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode all inputs and outputs\n",
    "language = Language(tokens, CONFIG.max_output_len)\n",
    "X = np.array([language.encode(question) for question in questions])\n",
    "Y = np.array([language.encode(question) for answer in answers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validation split\n",
    "X_train, X_val, Y_train, Y_val, questions_train, questions_val, answers_train, answers_val = train_test_split(X, Y, questions, answers, test_size=0.1)\n",
    "# Generate sample weights\n",
    "sample_weights = generate_sample_weights(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_model(output_dim):\n",
    "    \"\"\"Generate the model\"\"\"\n",
    "    print('Building model...')\n",
    "    model = Sequential()\n",
    "    # Encoding layers\n",
    "    model.add(recurrent.LSTM(CONFIG.hidden_size, input_shape=(None, output_dim), kernel_initializer=CONFIG.initialization, return_sequences=True))\n",
    "    model.add(Dropout(CONFIG.amount_of_dropout))\n",
    "    model.add(recurrent.LSTM(CONFIG.hidden_size, input_shape=(None, output_dim), kernel_initializer=CONFIG.initialization, return_sequences=False))\n",
    "    model.add(Dropout(CONFIG.amount_of_dropout))\n",
    "    # Repeat hidden representation\n",
    "    model.add(RepeatVector(CONFIG.max_output_len))\n",
    "    # Decoding layers\n",
    "    model.add(recurrent.LSTM(CONFIG.hidden_size, return_sequences=True, kernel_initializer=CONFIG.initialization))\n",
    "    model.add(Dropout(CONFIG.amount_of_dropout))\n",
    "    model.add(recurrent.LSTM(CONFIG.hidden_size, return_sequences=True, kernel_initializer=CONFIG.initialization))\n",
    "    model.add(Dropout(CONFIG.amount_of_dropout))\n",
    "\n",
    "    # For each of step of the output sequence, decide which token should be chosen\n",
    "    model.add(TimeDistributed(Dense(output_dim, kernel_initializer=CONFIG.initialization)))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'], sample_weight_mode=\"temporal\")\n",
    "    print('Model successfully built')\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def print_predictions(model, output_language, X, X_text, Y_text):\n",
    "    \"\"\"Print out errors on selected examples\"\"\"\n",
    "    print()\n",
    "    for rowX, rowX_text, rowY_text in zip(X, X_text, Y_text):\n",
    "        preds = model.predict_classes(rowX[np.newaxis], verbose=0)\n",
    "        preds_probs = model.predict(rowX[np.newaxis], verbose=0)\n",
    "        q = ' '.join(rowX_text)\n",
    "        correct = ' '.join(rowY_text)\n",
    "        guess = ' '.join(output_language.decode(preds[0]))\n",
    "        confidences = output_language.most_likely_seq_probs(preds_probs[0])\n",
    "\n",
    "        print('Question:', q)\n",
    "        print('Answer:', correct)\n",
    "        print('Guess:', guess)\n",
    "        print('Confidences', confidences)\n",
    "        print('---')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnEpochEndCallback(Callback):\n",
    "    \"\"\"Execute this every end of epoch\"\"\"\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        mask = sample(range(len(X_val)), 50)\n",
    "        X_sample, X_text_sample, Y_text_sample = X_val[mask], np.array(questions_val)[mask], np.array(questions_val)[mask] \n",
    "        language = Language(tokens, CONFIG.max_output_len)\n",
    "        print_predictions(self.model, language, X_sample, X_text_sample, Y_text_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Model successfully built\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_10 (LSTM)               (None, None, 50)          1659800   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_3 (RepeatVecto (None, 10, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 10, 50)            20200     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 10, 50)            0         \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 10, 50)            20200     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 10, 50)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 10, 8248)          420648    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10, 8248)          0         \n",
      "=================================================================\n",
      "Total params: 2,141,048\n",
      "Trainable params: 2,141,048\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7428 samples, validate on 826 samples\n",
      "Epoch 1/500\n",
      "5800/7428 [======================>.......] - ETA: 61s - loss: 6.1215 - acc: 0.0851"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = generate_model(language.size)\n",
    "ON_EPOCH_END_CALLBACK = OnEpochEndCallback()\n",
    "STOP_CALLBACK = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=5)\n",
    "history = model.fit(x=X_train, y=Y_train, batch_size=CONFIG.batch_size, epochs=CONFIG.epochs, validation_data=(X_val, Y_val), sample_weight=sample_weights, callbacks=[ON_EPOCH_END_CALLBACK, STOP_CALLBACK])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
